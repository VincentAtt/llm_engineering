{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai = OpenAI()\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "# either use this key with requests or simply use the ollama package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "system_prompt = \"You are a great joyful assistant\"\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This code snippet uses Python's `yield from` statement along with a set comprehension to iterate over a collection of books and yield the authors' names.\n",
       "\n",
       "Hereâ€™s a breakdown of the components:\n",
       "\n",
       "1. **Set Comprehension**: \n",
       "   - `{book.get(\"author\") for book in books if book.get(\"author\")}` generates a set of unique authors from the `books` collection.\n",
       "   - `book.get(\"author\")` attempts to retrieve the value associated with the key `\"author\"` from each `book` dictionary.\n",
       "   - The `if book.get(\"author\")` filters out any books that do not have an author, ensuring that only books with a valid author value are included in the final set.\n",
       "\n",
       "2. **`yield from`**: \n",
       "   - The `yield from` statement is used to yield all values from the iterator returned by the expression on its right. In this case, it yields each unique author found in the set produced by the set comprehension.\n",
       "   - `yield from` simplifies the generator function by yielding each value from the iterable, rather than looping through it and yielding each one individually.\n",
       "\n",
       "### Why Use This Code?\n",
       "- **Efficiency**: Since a set is used, this code produces a collection of unique authors, eliminating duplicates across the books.\n",
       "- **Readability**: The use of set comprehension provides a concise way of gathering authors, making the code easier to read and understand.\n",
       "- **Generator Functionality**: Using `yield from` allows the function to act as a generator, enabling iteration over authors when called, which is more memory-efficient than collecting all authors in a list before returning them.\n",
       "\n",
       "### Example:\n",
       "If `books` is defined as follows:\n",
       "```python\n",
       "books = [\n",
       "    {\"title\": \"Book One\", \"author\": \"Author A\"},\n",
       "    {\"title\": \"Book Two\", \"author\": \"Author B\"},\n",
       "    {\"title\": \"Book Three\", \"author\": \"Author A\"},\n",
       "    {\"title\": \"Book Four\"}  # No author\n",
       "]\n",
       "```\n",
       "\n",
       "Running the provided code would yield:\n",
       "```\n",
       "'Author A'\n",
       "'Author B'\n",
       "```\n",
       "\n",
       "These are the unique authors, with duplicates removed and ignoring the `book` without an author."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "\n",
    "response = openai.chat.completions.create(model=MODEL_GPT, \n",
    "                                          messages=[{\"role\":\"system\", \"content\": system_prompt},\n",
    "                                                    {\"role\":\"user\", \"content\": question}])\n",
    "\n",
    "# additional available parameter : \n",
    "    # response_format={\"type\": \"json_object\"}, for a dictionnary result\n",
    "        # which then can be handled like this :\n",
    "        #result = json.loads(response.choices[0].message.content)\n",
    "\n",
    "#print(response.choices[0].message.content)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f91f3b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "To make it streaming, day5 explains you can add `stream = True` in `stream = openai.chat.completions.create()` and then \n",
    "```python\n",
    "response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), \n",
    "                             display_id=True)\n",
    "    for chunk in stream: #stream is the name of the completions.create result\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Breaking Down the Code**\n",
       "\n",
       "This line of code is written in Python and utilizes a feature called **generator expressions**, which allows us to create an iterable directly within the syntax of the `for` loop.\n",
       "\n",
       "Here's what each part does:\n",
       "\n",
       "- `{book.get(\"author\") for book in books if book.get(\"author\")}`: This is a generator expression that:\n",
       "  - Iterates over a collection (in this case, `books`) using the `for book in books` syntax.\n",
       "  - Filters out any items in the collection for which the key `\"author\"` does not exist or its value is empty (`book.get(\"author\") == None` or `book.get(\"author\") == \"\"`). This is done using the `if book.get(\"author\")` condition.\n",
       "  - For each valid item, it extracts and returns the corresponding value.\n",
       "\n",
       "- `yield from {...}`: This keyword is used to delegate the iteration to another iterable (in this case, the generator expression above). It tells Python to yield each result one by one, instead of building an entire list or array in memory.\n",
       "\n",
       "**What Does It Do?**\n",
       "\n",
       "When you put it all together, this line of code will create a new iterator that yields only the values corresponding to the `\"author\"` key for each item in the `books` collection where the `\"author\"` key exists. The resulting iterable is then delegated to Python's built-in iteration mechanism.\n",
       "\n",
       "In essence, it performs a **lazy filtering** operation on the `books` collection, which can be beneficial when working with large datasets or memory-constrained environments.\n",
       "\n",
       "**Example Use Case**\n",
       "\n",
       "Here's an example of how this might be used in practice:\n",
       "\n",
       "```python\n",
       "# Assuming books is a list of dictionaries, where each dictionary represents a book.\n",
       "books = [\n",
       "    {\"title\": \"Book 1\", \"author\": \"Author A\"},\n",
       "    {\"title\": \"Book 2\", \"author\": None},\n",
       "    {\"title\": \"Book 3\", \"author\": \"Author C\"}\n",
       "]\n",
       "\n",
       "# Use the generator expression\n",
       "for author in yield from {book.get(\"author\") for book in books if book.get(\"author\")}:\n",
       "    print(author)  # Output: Author A, Author C\n",
       "\n",
       "# Compare to using a list comprehension and iterating over it\n",
       "authors = [book[\"author\"] for book in books if \"author\" in book]\n",
       "for author in authors:\n",
       "    print(author)  # Output: Author A, Author C\n",
       "```\n",
       "\n",
       "In the first example, we use `yield from` to create an iterator that produces only the authors of the books with existing `\"author\"` keys. In contrast, the second example uses a list comprehension to filter and store all valid authors in memory before iterating over them. The former approach is more memory-efficient for large datasets."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "payload = {\n",
    "        \"model\": MODEL_LLAMA,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": True\n",
    "    }\n",
    "\n",
    "# OLLAMA requests\n",
    "#response = requests.post(OLLAMA_API, json=payload, headers=HEADERS).json()\n",
    "\n",
    "# OLLAMA package\n",
    "response = ollama.chat(model=MODEL_LLAMA, messages=messages)\n",
    "\n",
    "display(Markdown(response['message']['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e3ee29",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "day2_exercize shows yet another possibility : using the OpenAI client python library to call Ollama.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db538ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=MODEL_LLAMA,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}


# Content

- day1 : jokes comparison and adversarial conversation
- day2 : gradio UI



# Day 1 : comparing models telling jokes

- Temperature : from fully deterministic (0) to fully random and creative (1)

- openai `completions` = continuer la conv
On peut parametrer de creer plusieurs choices, sinon juste `choices[0]` disponible

- Prompts usually works with `"role"`s defined in parameter `messages` : `system`, `user` and `assistant`

- with Claude.anthropic, the system and user messages are separate as follows :

```python
    system=system_message,
    messages=[{"role": "user", "content": user_prompt},]
```

You can also stream the answer, using `result = claude.messages.stream()` (and not `claude.messages.create()`), followed by :

```python
with result as stream:
    for text in stream.text_stream:
            print(text, end="", flush=True)
```

## Adversarial conversation between chatbots

Based an the usual prompt list structure :

```
[
    {"role": "system", "content": "system message here"},
    {"role": "user", "content": "user prompt here"}
]
```
that can be completed to reflect a longer conversation :

```
[
    {"role": "system", "content": "system message here"},
    {"role": "user", "content": "first user prompt here"},
    {"role": "assistant", "content": "the assistant's response"},
    {"role": "user", "content": "the new user prompt"},
]
```

Actually, that is what happens when you discuss with an LLM chatbot : the whole conversation feeds the next word prediction.

SO you can define GPT and Claude system prompts and functions `call_gpt()` and `call_claude()` and build a conversation !

# Day2 : Gradio

Acquired by HuggingFace

> `gradio` fonctionne avec websockets. La derniÃ¨re version 14.1 semble incomaptible alors sur les conseils de chatGPT, on a rÃ©trogradÃ© : `pip install websockets==10.4`. A priori, pour rÃ©-actualiser, `pip install --upgrade gradio`

You cannot really stream a message on gradio, but you can fake this in `yield`ing result in a look on chunks.

---

En Python, les fonctions `print`, `return`, et `yield` servent des objectifs trÃ¨s diffÃ©rents. Voici une explication dÃ©taillÃ©e de chacune :

---

## `yield` in a function, compared to `print()` and `return`

### **1. `print`**
- **Objectif :** Afficher du texte ou des donnÃ©es dans la console.
- **Utilisation typique :** `print` est utilisÃ© pour montrer des informations pendant l'exÃ©cution d'un programme (souvent Ã  des fins de dÃ©bogage ou pour fournir des sorties visibles Ã  l'utilisateur).
  
**Exemple :**
```python
def afficher_message():
    print("Bonjour, ceci est un message.")
afficher_message()  # Affichera : Bonjour, ceci est un message.
```

- **CaractÃ©ristiques :**
  - Il ne modifie pas ou ne retourne pas de donnÃ©es dans le programme.
  - La valeur retournÃ©e par `print` est toujours `None`.

---

### **2. `return`**
- **Objectif :** Renvoie une valeur (ou plusieurs) d'une fonction Ã  l'appelant.
- **Utilisation typique :** On l'utilise pour transmettre des rÃ©sultats calculÃ©s par une fonction Ã  une autre partie du programme.
  
**Exemple :**
```python
def ajouter(a, b):
    return a + b

resultat = ajouter(5, 7)  # La fonction retourne 12.
print(resultat)           # Affichera : 12
```

- **CaractÃ©ristiques :**
  - Une fois qu'une fonction rencontre un `return`, son exÃ©cution s'arrÃªte immÃ©diatement.
  - Les donnÃ©es renvoyÃ©es peuvent Ãªtre rÃ©utilisÃ©es ou manipulÃ©es ailleurs dans le code.

---

### **3. `yield`**
- **Objectif :** Permet de produire des valeurs (une par une) Ã  partir d'une fonction appelÃ©e gÃ©nÃ©rateur, sans perdre l'Ã©tat de la fonction.
- **Utilisation typique :** Utile pour produire des sÃ©quences ou des flux de donnÃ©es de maniÃ¨re paresseuse (lazy evaluation) et efficace en mÃ©moire.

**Exemple :**
```python
def generateur():
    for i in range(3):
        yield i

for valeur in generateur():
    print(valeur)  # Affichera : 0, puis 1, puis 2
```

- **CaractÃ©ristiques :**
  - Contrairement Ã  `return`, `yield` ne termine pas la fonction immÃ©diatement. La fonction peut reprendre son exÃ©cution Ã  l'endroit oÃ¹ elle s'est arrÃªtÃ©e.
  - Les fonctions utilisant `yield` sont des gÃ©nÃ©rateurs. Elles renvoient un **itÃ©rable** au lieu d'une seule valeur.
  - TrÃ¨s utile pour traiter des grandes quantitÃ©s de donnÃ©es sans les charger toutes en mÃ©moire.

---

### **DiffÃ©rences clÃ©s**

| **Aspect**        | **print**                          | **return**                         | **yield**                          |
|--------------------|------------------------------------|------------------------------------|------------------------------------|
| **But principal** | Afficher dans la console.          | Retourner une valeur.              | GÃ©nÃ©rer des valeurs une par une.  |
| **Sortie**         | `None`                            | Une valeur ou plusieurs.           | Un gÃ©nÃ©rateur/itÃ©rable.           |
| **Utilisation**    | Interaction ou dÃ©bogage.          | Passer des donnÃ©es au programme.   | ItÃ©ration paresseuse (lazy).      |
| **ArrÃªte la fonction ?** | Non.                          | Oui, aprÃ¨s l'exÃ©cution du `return`.| Non, la fonction peut continuer.  |

---

Day2 exercise consists in implementing our company prochure tool in a gradio UI !
He suggests to do it in the end of W1D5 code but I'll do a new W2D2 code.

## Gradio tips

### several inputs :

Pour ajouter deux entrÃ©es dans une application Gradio en Python, vous devez dÃ©finir deux widgets d'entrÃ©e dans le composant `gr.Interface`. Voici un exemple simple :

Cet exemple montre comment crÃ©er une application Gradio avec deux champs d'entrÃ©e, un champ texte et un nombre, et afficher leur rÃ©sultat combinÃ©.

```python
import gradio as gr

# Fonction de traitement
def combine_inputs(input1, input2):
    return f"Texte: {input1}, Nombre: {input2}"

# DÃ©finir les composants d'entrÃ©e et de sortie
input1 = gr.Textbox(label="Entrez un texte")
input2 = gr.Number(label="Entrez un nombre")
output = gr.Textbox(label="RÃ©sultat")

# Interface Gradio
interface = gr.Interface(
    fn=combine_inputs,  # Fonction Ã  appeler
    inputs=[input1, input2],  # Liste des entrÃ©es
    outputs=output       # Sortie
)

# Lancer l'application
interface.launch()
```

#### Explications
1. **Fonction principale** : La fonction `combine_inputs` prend deux paramÃ¨tres `input1` et `input2`. Ces paramÃ¨tres correspondent aux donnÃ©es saisies par l'utilisateur dans les deux champs.
   
2. **DÃ©finition des entrÃ©es et sorties** :
   - `Textbox` : Pour saisir du texte.
   - `Number` : Pour saisir un nombre.
   - `outputs` est dÃ©fini pour afficher le rÃ©sultat combinÃ©.

3. **Liaison dans l'interface** :
   - `inputs` est une liste contenant les deux composants d'entrÃ©e.
   - `outputs` correspond Ã  la sortie.

Quand vous lancez l'application avec `interface.launch()`, vous obtenez une interface avec deux champs d'entrÃ©e et un bouton pour exÃ©cuter la fonction.

### De la mise en page

Pour ajouter un joli titre Ã  votre application Gradio, vous pouvez utiliser le paramÃ¨tre `title` ou ajouter des composants `gr.Markdown` dans la mise en page avec une interface `gr.Blocks`. Voici deux approches :

---

### 1. Avec le paramÃ¨tre `title`
Gradio permet de dÃ©finir un titre directement en passant le paramÃ¨tre `title` dans `gr.Interface`.

```python
import gradio as gr

# Fonction de traitement
def combine_inputs(input1, input2):
    return f"Texte: {input1}, Nombre: {input2}"

# Interface Gradio avec un titre
interface = gr.Interface(
    fn=combine_inputs,  # Fonction Ã  appeler
    inputs=[gr.Textbox(label="Entrez un texte"), gr.Number(label="Entrez un nombre")],
    outputs=gr.Textbox(label="RÃ©sultat"),
    title="Ma Super Application Gradio"  # Ajout du titre
)

# Lancer l'application
interface.launch()
```
Le paramÃ¨tre `title` dans `gr.Interface` ajoute un titre simple au-dessus de votre interface.
---

#### 2. Avec `gr.Blocks` pour une personnalisation avancÃ©e
Si vous voulez personnaliser davantage la mise en page, comme ajouter une description ou styliser le titre, utilisez `gr.Blocks`. Cela vous permet d'ajouter des Ã©lÃ©ments Markdown.

```python
import gradio as gr

# Fonction de traitement
def combine_inputs(input1, input2):
    return f"Texte: {input1}, Nombre: {input2}"

# Construire l'interface avec Blocks
with gr.Blocks() as demo:
    gr.Markdown("# ðŸŒŸ Bienvenue sur Ma Super Application ðŸŒŸ")  # Titre stylisÃ© en Markdown
    gr.Markdown("Cette application combine un texte et un nombre, et affiche le rÃ©sultat.")
    with gr.Row():
        input1 = gr.Textbox(label="Entrez un texte")
        input2 = gr.Number(label="Entrez un nombre")
    output = gr.Textbox(label="RÃ©sultat")
    bouton = gr.Button("Lancer")
    
    # Ã‰vÃ©nement pour le bouton
    bouton.click(combine_inputs, inputs=[input1, input2], outputs=output)

# Lancer l'application
demo.launch()
```
- `gr.Markdown` permet d'Ã©crire des titres avec Markdown, ce qui permet des styles plus avancÃ©s (par exemple, des emojis, des couleurs, etc.).
   - Vous pouvez Ã©galement ajouter une description ou d'autres Ã©lÃ©ments visuels.

#### RÃ©sultat attendu avec `gr.Blocks`
Vous verrez :
- Un titre en grand avec des emojis et du texte Markdown.
- Une description sous le titre.
- Deux champs d'entrÃ©e alignÃ©s cÃ´te Ã  cÃ´te.
- Un bouton pour exÃ©cuter la fonction.

# Day 3 : User Interfaces and Chatbots - AI assistant on gradio

### Context and Progress in Chatbots
- Modern chatbots enable informed and contextual conversations, unlike older systems limited to predefined choices.
- LLMs can:
  - Maintain conversation history by including all context in the prompt.
  - Adopt **custom personas**.
  - Demonstrate subject matter expertise to answer questions competently.

### Key Techniques for Interacting with LLMs
1. **System Prompt**: Defines the tone of the conversation and sets rules.
   - Example: "If you donâ€™t know the answer, just say so."
2. **Context**: Adds additional information to enhance responses.
3. **Multi-shot Prompting**: Provides multiple examples in the prompt to guide the modelâ€™s behavior.
   - Helps shape the LLM by offering response patterns.
   - Differs from traditional training as it happens during inference (runtime).

Reminder : 
- LLMs generate responses by predicting the next tokens based on the provided history.
- The quality of prompts significantly impacts model performance.

### Notes

About Gradio, we can use the ChatInterface function.

Originally, we would have had to make functions that allows both an LLM model and a gradio app to correctly work together. But Gradio upgrade to adapt to OpenAI.

We write a function `chat(message, history)` where:  
**message** is the prompt to use  
**history** is the past conversation, in OpenAI format

We combine the system message, history and latest message, then call OpenAI.

> About the structure of successive roles "system"/"user"/"asssistant" in a dictionnary, Ed explains that like everything else, LLMs understand tham as tokens but that the structure is automotically considered as mark-ups for a new step. It is not "implemented" : LLM have been trained like this !

```python
system_message = "You are a helpful assistant in a clothes store. You should try to gently encourage \
the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. \
For example, if the customer says 'I'm looking to buy a hat', \
you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales evemt.'\
Encourage the customer to buy hats if they are unsure what to get."
```

Notice that we provide facts and examples. Examples allows us to introduce both tone and style. For instance, the chat can be very enthusiastic to sell more items.

You can add options such as : 

```python
if 'belt' in message:  #highly improvable
    relevant_system_message += " The store does not sell belts; if you are asked for belts, be sure to point out other items on sale."
```

OR including messages that have not actually happened in the beginning, to improve the chatbot through previous answers from the assitant.

RAG is about finding such extra information that is relevant to the prompt and adding it in the context, in a more sophisticated way.

I chose to answer the exercise in switching the prompt so the bot encourages customers to be eco-friendly.

